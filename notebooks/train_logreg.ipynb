{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv    \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "x289woU0jdaF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.read_csv('trex_embeddings_20000.csv', \n",
    "                      header=None, \n",
    "                      names=['embedding', 'triplet', 'rel_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding</th>\n",
       "      <th>triplet</th>\n",
       "      <th>rel_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[6.22067600e-02 1.60363363e-03 3.17481868e-02 ...</td>\n",
       "      <td>('Hell', 'outcast', 'Devilkin', '0')</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.1821641e-02 2.5162415e-03 1.7185923e-02 1.9...</td>\n",
       "      <td>('Dungeons &amp; Dragons', 'roleplaying game', 'is...</td>\n",
       "      <td>P31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2.41272449e-01 1.46490023e-01 1.23494724e-02 ...</td>\n",
       "      <td>('England', 'Hartfield', 'Sussex', '0')</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4.42590602e-02 1.16817303e-01 8.24852288e-02 ...</td>\n",
       "      <td>('Hartfield', 'civil parish', 'is a', 'P31')</td>\n",
       "      <td>P31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[4.13215496e-02 6.18278806e-04 1.09743709e-02 ...</td>\n",
       "      <td>('area', 'miles', 'is', '0')</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           embedding  \\\n",
       "0  [6.22067600e-02 1.60363363e-03 3.17481868e-02 ...   \n",
       "1  [1.1821641e-02 2.5162415e-03 1.7185923e-02 1.9...   \n",
       "2  [2.41272449e-01 1.46490023e-01 1.23494724e-02 ...   \n",
       "3  [4.42590602e-02 1.16817303e-01 8.24852288e-02 ...   \n",
       "4  [4.13215496e-02 6.18278806e-04 1.09743709e-02 ...   \n",
       "\n",
       "                                             triplet rel_label  \n",
       "0               ('Hell', 'outcast', 'Devilkin', '0')         0  \n",
       "1  ('Dungeons & Dragons', 'roleplaying game', 'is...       P31  \n",
       "2            ('England', 'Hartfield', 'Sussex', '0')         0  \n",
       "3       ('Hartfield', 'civil parish', 'is a', 'P31')       P31  \n",
       "4                       ('area', 'miles', 'is', '0')         0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  костыльное решение того что я плохо записываю np array в csv\n",
    "def read_array(emb: str):\n",
    "    new_array = []\n",
    "    for row in emb.strip('[]').split('\\n'):\n",
    "        for dig in row.split():\n",
    "            new_array.append(float(dig))\n",
    "            \n",
    "    return np.array(new_array).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.embedding = embeddings_df.embedding.apply(read_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = np.array(list(embeddings_df.embedding.values)).squeeze()\n",
    "labels = embeddings_df.rel_label.values\n",
    "\n",
    "assert len(embs) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels = ['0' if label == '0' else '1' for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0': 38595, '1': 35918})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(binary_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бинарная классификация (мусор и не-мусор)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(embs, binary_labels,\n",
    "                                                    stratify=binary_labels, \n",
    "                                                    test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bin = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "\n",
    "with open('logreg_bin.pkl', 'wb') as file:\n",
    "      pickle.dump(lr_bin, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87     11579\n",
      "           1       0.86      0.85      0.86     10775\n",
      "\n",
      "    accuracy                           0.86     22354\n",
      "   macro avg       0.86      0.86      0.86     22354\n",
      "weighted avg       0.86      0.86      0.86     22354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, lr_bin.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многоклассовая классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  оставляем только классы, встретившиеся больше 10 раз\n",
    "labels_counter = Counter(labels)\n",
    "labels_over10 = [label for label in labels_counter if labels_counter[label] > 10]\n",
    "\n",
    "new_embs = np.array(list(embeddings_df[embeddings_df.rel_label.isin(labels_over10)].embedding.values)).squeeze()\n",
    "new_labels = embeddings_df[embeddings_df.rel_label.isin(labels_over10)].rel_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  убираем мусорный класс\n",
    "ros = RandomUnderSampler(sampling_strategy={'0': 0})\n",
    "X_resampled, y_resampled = ros.fit_resample(new_embs, new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35446"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_resampled)\n",
    "y = np.array(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = next(iter(StratifiedKFold().split(X_resampled, y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[train], X[test]\n",
    "y_train, y_test = y[train], y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight='balanced', max_iter=1000).fit(X_train, y_train)\n",
    "    \n",
    "with open('logreg_multi.pkl', 'wb') as file:\n",
    "      pickle.dump(lr, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        P102       0.04      0.43      0.07         7\n",
      "        P106       0.65      0.76      0.70        29\n",
      "       P1066       0.33      0.20      0.25         5\n",
      "        P112       0.52      0.40      0.45        30\n",
      "        P113       0.25      1.00      0.40         2\n",
      "        P115       0.61      0.68      0.64        25\n",
      "        P118       0.72      0.81      0.76        42\n",
      "        P119       1.00      0.50      0.67         4\n",
      "       P1191       1.00      0.33      0.50         3\n",
      "        P123       0.14      0.33      0.20         3\n",
      "        P127       0.67      0.31      0.42        26\n",
      "       P1303       0.50      0.83      0.62         6\n",
      "        P131       0.95      0.72      0.82      1308\n",
      "       P1336       0.10      0.50      0.17         2\n",
      "       P1344       0.14      1.00      0.24         3\n",
      "       P1346       0.50      0.30      0.37        10\n",
      "        P135       0.08      0.33      0.13         3\n",
      "        P136       0.70      0.64      0.67        11\n",
      "        P137       0.54      0.50      0.52        26\n",
      "       P1376       0.63      0.83      0.72        48\n",
      "        P138       0.93      0.54      0.68        24\n",
      "        P140       0.07      0.25      0.11         4\n",
      "       P1411       0.68      1.00      0.81        17\n",
      "        P144       0.06      0.33      0.11         6\n",
      "        P150       0.04      0.33      0.07         9\n",
      "        P156       0.05      0.50      0.09         6\n",
      "        P161       0.85      0.76      0.80       188\n",
      "       P1619       1.00      1.00      1.00         3\n",
      "        P162       0.12      0.25      0.17         4\n",
      "        P166       0.92      0.58      0.71        57\n",
      "        P169       0.43      1.00      0.60         3\n",
      "         P17       0.96      0.72      0.82       723\n",
      "        P170       0.38      0.23      0.29        22\n",
      "        P175       0.76      0.73      0.74        89\n",
      "        P176       0.64      0.30      0.41        23\n",
      "        P177       0.25      1.00      0.40         5\n",
      "        P178       0.53      0.31      0.39        29\n",
      "        P179       0.80      0.64      0.71        25\n",
      "        P186       0.00      0.00      0.00         5\n",
      "         P19       0.99      0.86      0.92       201\n",
      "        P194       0.41      0.78      0.54         9\n",
      "         P20       0.96      0.95      0.96        79\n",
      "       P2176       0.24      0.80      0.36         5\n",
      "         P22       0.30      0.26      0.28        34\n",
      "         P25       0.17      0.50      0.25        10\n",
      "         P26       0.77      0.53      0.63       140\n",
      "        P264       0.78      0.58      0.67        67\n",
      "         P27       0.14      0.50      0.22         4\n",
      "        P276       0.27      0.57      0.36         7\n",
      "        P279       0.20      0.50      0.29        28\n",
      "        P282       0.18      0.50      0.27         4\n",
      "        P286       0.11      0.33      0.17         3\n",
      "        P287       0.00      0.00      0.00         3\n",
      "         P30       0.07      0.67      0.13         3\n",
      "        P306       0.33      0.25      0.29         4\n",
      "         P31       1.00      0.78      0.88      1548\n",
      "        P344       0.33      1.00      0.50         3\n",
      "         P36       0.62      0.64      0.63        77\n",
      "        P361       0.29      0.36      0.32        66\n",
      "        P366       0.00      0.00      0.00         4\n",
      "         P37       0.13      1.00      0.24         4\n",
      "         P38       0.23      0.75      0.35         4\n",
      "         P40       0.87      0.52      0.65       175\n",
      "        P400       0.14      0.83      0.23         6\n",
      "        P403       0.56      0.61      0.58        23\n",
      "        P449       0.41      0.41      0.41        17\n",
      "        P451       0.00      0.00      0.00         3\n",
      "        P463       0.63      0.66      0.64        29\n",
      "         P47       0.96      0.70      0.81       583\n",
      "        P488       0.16      0.44      0.24         9\n",
      "         P50       0.55      0.33      0.41        51\n",
      "        P527       0.28      0.10      0.15        80\n",
      "         P54       0.94      0.68      0.79       315\n",
      "        P569       0.80      0.96      0.87        25\n",
      "         P57       0.89      0.53      0.66       146\n",
      "        P570       0.96      0.81      0.88        68\n",
      "        P571       0.38      0.73      0.50        15\n",
      "        P577       1.00      0.95      0.97        92\n",
      "         P58       0.25      0.80      0.38         5\n",
      "        P580       0.19      0.75      0.30         4\n",
      "         P59       0.44      1.00      0.62         4\n",
      "          P6       0.08      0.67      0.14         3\n",
      "        P607       0.16      0.75      0.27        16\n",
      "         P61       0.70      0.88      0.78         8\n",
      "        P610       0.12      0.83      0.21         6\n",
      "        P641       0.79      0.89      0.84        38\n",
      "        P647       0.10      0.50      0.17         2\n",
      "        P676       0.07      0.67      0.13         3\n",
      "         P69       0.97      0.88      0.92       108\n",
      "        P706       0.68      0.70      0.69        30\n",
      "        P710       0.00      0.50      0.01         2\n",
      "        P740       0.00      0.00      0.00         4\n",
      "        P749       0.27      0.67      0.39         9\n",
      "        P750       1.00      0.80      0.89         5\n",
      "        P800       0.07      0.55      0.13        22\n",
      "        P802       0.06      0.14      0.09         7\n",
      "        P828       0.04      0.33      0.06         3\n",
      "         P84       0.35      0.75      0.48         8\n",
      "         P86       0.18      0.27      0.22        15\n",
      "         P87       0.00      0.00      0.00         3\n",
      "        P974       0.08      1.00      0.15         3\n",
      "         P98       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.70      7090\n",
      "   macro avg       0.43      0.57      0.43      7090\n",
      "weighted avg       0.87      0.70      0.76      7090\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
