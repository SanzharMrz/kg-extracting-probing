{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations, product\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation += '’'\n",
    "string.punctuation += '–'\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "bert_model = bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(sentence, tokenizer, nlp, return_pt=True):\n",
    "    doc = nlp(sentence)\n",
    "    tokens = list(doc)\n",
    "\n",
    "    chunk2id = {}\n",
    "\n",
    "    start_chunk = []\n",
    "    end_chunk = []\n",
    "    noun_chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        noun_chunks.append(chunk.text)\n",
    "        start_chunk.append(chunk.start)\n",
    "        end_chunk.append(chunk.end)\n",
    "\n",
    "    sentence_mapping = []\n",
    "    token2id = {}\n",
    "    mode = 0 # 1 in chunk, 0 not in chunk       \n",
    "    chunk_id = 0\n",
    "    for idx, token in enumerate(doc):\n",
    "        if idx in start_chunk:\n",
    "            mode = 1\n",
    "            sentence_mapping.append(noun_chunks[chunk_id])\n",
    "            if sentence_mapping[-1] not in token2id:\n",
    "                token2id[sentence_mapping[-1]] = len(token2id)\n",
    "            chunk_id += 1\n",
    "        elif idx in end_chunk:\n",
    "            mode = 0\n",
    "\n",
    "        if mode == 0:\n",
    "            sentence_mapping.append(token.text)\n",
    "            if sentence_mapping[-1] not in token2id:\n",
    "                token2id[sentence_mapping[-1]] = len(token2id)\n",
    "\n",
    "\n",
    "    token_ids = []\n",
    "    tokenid2word_mapping = []\n",
    "\n",
    "    for token in sentence_mapping:\n",
    "        subtoken_ids = tokenizer(str(token), add_special_tokens=False)['input_ids']\n",
    "        tokenid2word_mapping += [ token2id[token] ]*len(subtoken_ids)\n",
    "        token_ids += subtoken_ids\n",
    "\n",
    "    tokenizer_name = str(tokenizer.__str__)\n",
    "    if 'GPT2' in tokenizer_name:\n",
    "        outputs = {\n",
    "            'input_ids': token_ids,\n",
    "            'attention_mask': [1]*(len(token_ids)),\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        outputs = {\n",
    "            'input_ids': [tokenizer.cls_token_id] + token_ids + [tokenizer.sep_token_id],\n",
    "            'attention_mask': [1]*(len(token_ids)+2),\n",
    "            'token_type_ids': [0]*(len(token_ids)+2)\n",
    "        }\n",
    "\n",
    "    if return_pt:\n",
    "        for key, value in outputs.items():\n",
    "            outputs[key] = torch.from_numpy(np.array(value)).long().unsqueeze(0)\n",
    "    \n",
    "    return outputs, tokenid2word_mapping, token2id, noun_chunks, sentence_mapping\n",
    "\n",
    "\n",
    "def compress_attention(attention, tokenid2word_mapping, operator=np.mean):\n",
    "\n",
    "    new_index = []\n",
    "    \n",
    "    prev = -1\n",
    "    for idx, row in enumerate(attention):\n",
    "        token_id = tokenid2word_mapping[idx]\n",
    "        if token_id != prev:\n",
    "            new_index.append( [row])\n",
    "            prev = token_id\n",
    "        else:\n",
    "            new_index[-1].append(row)\n",
    "\n",
    "    new_matrix = []\n",
    "    for row in new_index:\n",
    "        new_matrix.append(operator(np.array(row), 0))\n",
    "\n",
    "    new_matrix = np.array(new_matrix)\n",
    "\n",
    "    attention = np.array(new_matrix).T\n",
    "\n",
    "    prev = -1\n",
    "    new_index=  []\n",
    "    for idx, row in enumerate(attention):\n",
    "        token_id = tokenid2word_mapping[idx]\n",
    "        if token_id != prev:\n",
    "            new_index.append( [row])\n",
    "            prev = token_id\n",
    "        else:\n",
    "            new_index[-1].append(row)\n",
    "\n",
    "    \n",
    "    new_matrix = []\n",
    "    for row in new_index:\n",
    "        new_matrix.append(operator(np.array(row), 0))\n",
    "    \n",
    "    new_matrix = np.array(new_matrix)\n",
    "    \n",
    "    return new_matrix.T\n",
    "\n",
    "\n",
    "def get_outputs(sentence, tokenizer, encoder, nlp, use_cuda=False):\n",
    "\n",
    "    tokenizer_name = str(tokenizer.__str__)\n",
    "    inputs, tokenid2word_mapping, token2id, tokens, sentence_mapping = process(sentence, nlp=nlp, tokenizer=tokenizer, return_pt=True)\n",
    "    id2token = {value: key for key, value in token2id.items()}\n",
    "    for key in inputs.keys():\n",
    "        inputs[key] = inputs[key].to(device)\n",
    "    outputs = encoder(**inputs, output_attentions=True)\n",
    "    \n",
    "    return outputs[2], tokenid2word_mapping, token2id, sentence_mapping\n",
    "\n",
    "\n",
    "def get_embeddings(sentence):\n",
    "    rel_pos = ['NN', 'NNP', 'NNS', 'JJR', 'JJS', 'MD', 'POS', 'VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ']\n",
    "    head_tail_pos = ['NN', 'NNP', 'NNS', 'PRP']\n",
    "\n",
    "    use_cuda = True    \n",
    "    att, tokenid2word_mapping, token2id, sentence_mapping = get_outputs(sentence, bert_tokenizer, bert_model, nlp, use_cuda=use_cuda)\n",
    "    \n",
    "    new_matr = []\n",
    "    \n",
    "    for layer in att:\n",
    "        for head in layer.squeeze():\n",
    "            attn = head.cpu()\n",
    "            attention_matrix = attn.detach().numpy()\n",
    "            attention_matrix = attention_matrix[1:-1, 1:-1]\n",
    "            \n",
    "            merged_attention = compress_attention(attention_matrix, tokenid2word_mapping)\n",
    "            \n",
    "            new_matr.append(merged_attention)\n",
    "    \n",
    "    new_matr = np.stack(new_matr)\n",
    "    \n",
    "    words = [token for token in sentence_mapping if token not in string.punctuation]\n",
    "    \n",
    "    nn_words = [word for word in words if nltk.pos_tag([word])[0][1] in head_tail_pos]\n",
    "    other_words = [word for word in words if nltk.pos_tag([word])[0][1] in rel_pos]\n",
    "    \n",
    "    triplets = [triplet for triplet in list(product(nn_words, nn_words, other_words)) \n",
    "                if triplet[0] != triplet[1] and triplet[0] != triplet[2] and triplet[1] != triplet[2]]\n",
    "    \n",
    "    sent_embeddings = []\n",
    "    \n",
    "    for triplet in triplets:\n",
    "       \n",
    "        head_ind = sentence_mapping.index(triplet[0])\n",
    "        tail_ind = sentence_mapping.index(triplet[1])\n",
    "        rel_ind = sentence_mapping.index(triplet[2])   \n",
    "\n",
    "        head_rel_emb = new_matr[:, head_ind, rel_ind]\n",
    "        rel_tail_emb = new_matr[:, rel_ind, tail_ind]\n",
    "\n",
    "        triplet_emb = np.concatenate((head_rel_emb, rel_tail_emb), axis=0).squeeze()\n",
    "        sent_embeddings.append((triplet_emb, triplet))\n",
    "        \n",
    "    \n",
    "    return sent_embeddings\n",
    "\n",
    "\n",
    "def deduplication(pred_list):\n",
    "    pred_max_conf = {}\n",
    "    filtered_pred = {}\n",
    "\n",
    "    for ind, pred in enumerate(pred_list):\n",
    "        pred_triplet = (pred[1][0], pred[1][1])\n",
    "\n",
    "        if pred_triplet not in filtered_pred.keys():\n",
    "            pred_max_conf[pred_triplet] = pred[2]\n",
    "            filtered_pred[pred_triplet] = pred\n",
    "\n",
    "        elif pred_triplet in filtered_pred and pred[2] > pred_max_conf[pred_triplet]:\n",
    "            pred_max_conf[pred_triplet] = pred[2]\n",
    "            filtered_pred[pred_triplet] = pred\n",
    "    \n",
    "    sorted_pred = sorted(list(filtered_pred.values()), key=lambda x: x[2], reverse=True)\n",
    "    prediction = [el[1] for el in sorted_pred]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "\n",
    "def get_predictions(sentence, threshold_bin=0.5):\n",
    "    pred_list = []\n",
    "    emb_sent = get_embeddings(sentence)\n",
    "    for emb in emb_sent:\n",
    "        binary_conf = lr_bin.predict_proba(emb[0].reshape(1, -1))[0][1]\n",
    "        if binary_conf > threshold_bin:\n",
    "            predicted_label = list(lr_multi.predict(emb[0].reshape(1, -1)))[0]\n",
    "            triplet = emb[1]\n",
    "            pred_list.append((predicted_label, triplet, binary_conf))\n",
    "    return pred_list\n",
    "\n",
    "\n",
    "def compare_triplets(targets, predict, dist_thresh=0.4):\n",
    "    compare_result = []\n",
    "    for target in targets:\n",
    "        sub_compare = []\n",
    "        for target, predict_ in zip(target, predict):\n",
    "            answer =  False\n",
    "            dist = jaccard_distance(set(target.lower()), set(predict_.lower()))\n",
    "            if predict_ in target or dist < dist_thresh or target in predict_:\n",
    "                answer = True\n",
    "            sub_compare.append(answer)\n",
    "        sub_compare = all(sub_compare)\n",
    "        compare_result.append(sub_compare)\n",
    "    return any(compare_result)    \n",
    "\n",
    "\n",
    "def compute_logreg_nm(dataset):\n",
    "    fp, tp, fn = 0, 0, 0\n",
    "    tp_predicts_dict = {}\n",
    "    for row in dataset.itertuples():\n",
    "        \n",
    "        try:\n",
    "            predictions = get_predictions(row.text, threshold_bin=0.7)\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        filtered_predictions = deduplication(predictions)\n",
    "        print(filtered_predictions)\n",
    "        target_triplets = [target[:3] for target in eval(row.target)]\n",
    "        print('!!! target',target_triplets)\n",
    "        tp_predicts = []\n",
    "        for predict in filtered_predictions:\n",
    "            \n",
    "            score_bool = compare_triplets(target_triplets, predict)\n",
    "\n",
    "            if score_bool:\n",
    "                tp_predicts.append(predict)\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "                \n",
    "        if len(tp_predicts):\n",
    "            tp_predicts_dict[row.text] = tp_predicts\n",
    "        \n",
    "        for target in target_triplets:\n",
    "            score_bool = compare_triplets(filtered_predictions, target)\n",
    "            if not score_bool:\n",
    "                fn += 1\n",
    "\n",
    "    try:        \n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return precision, recall, f1, tp_predicts_dict\n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        return 0, 0, 0, tp_predicts_dict\n",
    "\n",
    "\n",
    "def compute_csv_default(dataset, sample_size=100, filename='filename'):\n",
    "    rels, prs, rcls, f1s = [], [], [], []\n",
    "    sizes, labels, preds = [], [], []\n",
    "\n",
    "    for rel in tqdm(sorted(lr_multi.classes_, key=lambda x: int(x[1:]))):\n",
    "        mono_tr_subset = dataset[dataset.rel == rel]\n",
    "\n",
    "        if not mono_tr_subset.empty:\n",
    "            if mono_tr_subset.shape[0] > sample_size:\n",
    "                mono_tr_subset = mono_tr_subset.sample(sample_size)\n",
    "\n",
    "            label = get_title(rel)\n",
    "            size = mono_tr_subset.shape[0]\n",
    "            try:\n",
    "                precision, recall, f1, pred_dict = compute_logreg_nm(mono_tr_subset)\n",
    "            except:\n",
    "                print(f'rel {rel} skipped, check soon')\n",
    "                continue\n",
    "\n",
    "            rels.append(rel)\n",
    "            prs.append(precision)\n",
    "            rcls.append(recall)\n",
    "            f1s.append(f1)\n",
    "            sizes.append(size)\n",
    "            labels.append(label)\n",
    "            preds.append(pred_dict)\n",
    "\n",
    "            scoring_result = pd.DataFrame({'rel': rels,\n",
    "                                           'label': labels,\n",
    "                                           'size': sizes, \n",
    "                                           'precision': prs, \n",
    "                                           'recall': rcls, \n",
    "                                           'f1': f1s,\n",
    "                                           'tps': preds})\n",
    "\n",
    "            scoring_result.to_csv(f'{filename}.csv', index=False)\n",
    "\n",
    "def get_title(relation_id):\n",
    "    return RELATIONS[RELATIONS.relation==relation_id].title.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('P17', ('hi', 'my name', 'i'), 0.5986786860214062),\n",
       " ('P276', ('Sanzhar', 'my name', 'i'), 0.5261964351536904),\n",
       " ('P276', ('Almaty', 'my name', 'i'), 0.6590005936830667)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions('hi my name is Sanzhar i am from Almaty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Almaty', 'my name', 'i'),\n",
       " ('hi', 'my name', 'i'),\n",
       " ('Sanzhar', 'my name', 'i')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduplication(get_predictions('hi my name is Sanzhar i am from Almaty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logreg_multi.pkl', 'rb') as file:\n",
    "    lr_multi = pickle.load(file)\n",
    "    \n",
    "with open('logreg_bin.pkl', 'rb') as file:\n",
    "    lr_bin = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATIONS = pd.read_csv('../data/meta/relations_docred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/train-val-test/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'County Down', 'born'), ('He', 'Margaret Garrett', 'born'), ('He', 'William Kidd', 'born'), ('He', 'the son', 'born')]\n",
      "!!! target [('He', 'County Down', 'born in')]\n",
      "[]\n",
      "!!! target [('Jon Juaristi', 'Bilbao', 'born in')]\n",
      "[('He', 'a son', 'born'), ('He', 'Haapsalu', 'born'), ('He', 'Field', 'born'), ('He', 'Agneta von Dellwig', 'born'), ('He', 'Marshal Carl Horn', 'born')]\n",
      "!!! target [('He', 'Haapsalu', 'born in')]\n",
      "[('Trier', 'Rome', 'moved'), ('He', 'Rome', 'moved'), ('Germany', 'Rome', 'moved'), ('He', 'Germany', 'born'), ('a lawyer', 'Rome', 'moved'), ('He', 'Trier', 'born'), ('born', 'Rome', 'moved'), ('He', 'a lawyer', 'born')]\n",
      "!!! target [('He', 'Trier', 'born at')]\n",
      "[('He', 'Milan', 'born'), ('Politecnico di Milano', 'Milan', 'born'), ('he', 'Milan', 'born'), ('studied architecture', 'Milan', 'born')]\n",
      "!!! target [('He', 'Milan', 'born in')]\n",
      "[('Pierre-Gilles de Gennes', 'Paris', 'born')]\n",
      "!!! target [('Pierre-Gilles de Gennes', 'Paris, France', 'born in')]\n",
      "[('He', 'Chalkis', 'born'), ('He', 'an NCO', 'born')]\n",
      "!!! target [('He', 'Chalkis', 'born in')]\n",
      "[('He', 'Odense', 'born'), ('He', 'a jeweller', 'born'), ('He', 'the son', 'born')]\n",
      "!!! target [('He', 'Odense', 'born at')]\n",
      "[('He', 'Tours', 'born'), ('He', 'Indre-et-Loire', 'born'), ('He', 'Paris', 'born'), ('He', 'he', 'born'), ('He', 'the Lycée Henri IV', 'born'), ('He', 'fellow student Jean-Paul Sartre', 'born'), ('Indre-et-Loire', 'Paris', 'studied')]\n",
      "!!! target [('He', 'Tours', 'born in')]\n",
      "[(\"The city's name\", 'Ignacio Manuel Altamirano', 'honours'), (\"The city's name\", 'Guerrero', 'honours'), (\"The city's name\", 'writer', 'honours'), (\"The city's name\", 'a 19th-century president', 'honours'), (\"The city's name\", 'born', 'honours'), (\"The city's name\", 'Tixtla', 'honours'), ('a 19th-century president', 'Guerrero', 'born'), (\"The city's name\", 'the Supreme Court', 'honours'), ('the Supreme Court', 'Guerrero', 'born')]\n",
      "!!! target [('Ignacio Manuel Altamirano', 'Tixtla, Guerrero', 'born in')]\n",
      "[]\n",
      "!!! target [('She', 'Wuppertal', 'born in')]\n",
      "[('She', 'London', 'born'), ('She', 'Iranian parents', 'born')]\n",
      "!!! target [('She', 'London', 'born in')]\n",
      "[('He', 'Mannheim', 'born')]\n",
      "!!! target [('He', 'Mannheim', 'born at')]\n",
      "[('a Dutch biologist', 'Rotterdam', 'born'), ('naturalist', 'Rotterdam', 'born')]\n",
      "!!! target [('Pieter Harting', 'Rotterdam', 'born in')]\n",
      "[('She', 'Kumamoto', 'born')]\n",
      "!!! target [('She', 'Kumamoto', 'born in')]\n",
      "[('He', 'Como', 'born'), ('He', 'the Milanese Pietro di Martino', 'born'), ('He', 'Lombardy', 'born'), ('He', 'the late 15th century', 'born'), ('He', 'a pupil', 'born')]\n",
      "!!! target [('He', 'Como', 'born in')]\n",
      "[('He', 'South Dakota', 'born'), ('He', 'Southern California', 'born'), ('He', 'Pomona College', 'born'), ('He', 'Fine Arts', 'born'), ('He', 'the Claremont Graduate School', 'born'), ('He', 'his Bachelor', 'born'), ('He', 'his Master', 'born'), ('He', 'Arts', 'born'), ('his Master', 'South Dakota', 'born'), ('his Master', 'his Bachelor', 'receiving')]\n",
      "!!! target [('He', 'Watertown, South Dakota', 'born in')]\n",
      "[('He', 'Thomas Henry', 'the son'), ('He', 'Manchester England', 'born'), ('Manchester England', 'Thomas Henry', 'the son')]\n",
      "!!! target [('He', 'Manchester', 'born in')]\n",
      "[]\n",
      "!!! target [('Sabinian', 'Blera', 'born at')]\n",
      "[('He', 'Manuel', 'the older brother'), ('Eustaquio', 'Manuel', 'the older brother'), ('Mexico City', 'Manuel', 'the older brother'), ('He', 'Mexico City', 'born'), ('the older brother', 'Manuel', 'was'), ('born', 'Manuel', 'the older brother'), ('He', 'Eustaquio', 'born')]\n",
      "!!! target [('He', 'Mexico City', 'born in')]\n",
      "[('Lara Harris', 'Chicago', 'born'), ('actress', 'Chicago', 'born'), ('an American model', 'Chicago', 'born'), ('Lara Harris', 'an American model', 'is')]\n",
      "!!! target [('Lara Harris', 'Chicago', 'born in')]\n",
      "[('He', 'Sussex', 'born'), ('He', 'Wadhurst', 'born')]\n",
      "!!! target [('He', 'Wadhurst', 'born at')]\n",
      "[('He', 'Connecticut', 'born'), ('He', 'Mansfield', 'born'), ('Mansfield', 'Connecticut', 'was')]\n",
      "!!! target [('He', 'Mansfield, Connecticut', 'born in')]\n",
      "[('She', 'Manitoba', 'born'), ('She', 'Canada', 'born'), ('She', 'Starbuck', 'born')]\n",
      "!!! target [('She', 'Starbuck, Manitoba', 'born in')]\n",
      "[('He', 'Uttar Pradesh', 'born'), ('He', 'Vrindavan', 'born'), ('He', 'the Indian state', 'born'), ('He', 'Mathura district', 'born')]\n",
      "!!! target [('He', 'Vrindavan', 'born in')]\n",
      "[('Genelli', 'Berlin', 'born'), ('grandson', 'a school', 'found'), ('a Roman embroiderer', 'a school', 'found'), ('Joseph Genelli', 'a school', 'found'), ('tapestrys', 'Frederick', 'a school'), ('Berlin', 'a school', 'found'), ('Janus Genelli', 'a school', 'found'), ('the Schloss', 'a school', 'found'), ('Genelli', 'a school', 'found'), ('Frederick', 'Berlin', 'born'), ('a painter', 'a school', 'found'), ('Janus Genelli', 'Berlin', 'born'), ('the son', 'a school', 'found'), ('Joseph Genelli', 'Berlin', 'born'), ('Frederick', 'a school', 'found'), ('the Great', 'Berlin', 'born'), ('whose landscapes', 'a school', 'found'), ('born', 'a school', 'found'), ('a Roman embroiderer', 'Berlin', 'born'), ('tapestrys', 'Berlin', 'born'), ('the Great', 'a school', 'found'), ('the Schloss', 'Berlin', 'born'), ('Joseph Genelli', 'Frederick', 'a school'), ('Janus Genelli', 'Frederick', 'a school'), ('whose landscapes', 'Berlin', 'born'), ('a Roman embroiderer', 'Frederick', 'a school'), ('Genelli', 'Frederick', 'a school'), ('Berlin', 'Frederick', 'a school'), ('a painter', 'Berlin', 'born'), ('the Schloss', 'Frederick', 'a school'), ('the son', 'Frederick', 'a school'), ('born', 'Frederick', 'a school'), ('a painter', 'Frederick', 'a school'), ('whose landscapes', 'Frederick', 'a school'), ('grandson', 'Frederick', 'a school'), ('tapestrys', 'a school', 'found'), ('Genelli', 'the son', 'born'), ('found', 'Berlin', 'born'), ('the Great', 'Frederick', 'a school')]\n",
      "!!! target [('Frederick', 'Berlin', 'born at')]\n",
      "[('He', 'Germany', 'born'), ('He', 'Neubrandenburg', 'born')]\n",
      "!!! target [('He', 'Neubrandenburg', 'born in')]\n",
      "[('Glauert', 'Sheffield', 'born')]\n",
      "!!! target [('Glauert', 'Sheffield, England', 'born in')]\n",
      "[('He', 'Auckland', 'born'), ('He', 'New Zealand', 'born'), ('He', 'Devonport', 'born')]\n",
      "!!! target [('He', 'Devonport, Auckland', 'born in')]\n",
      "[('He', 'Paris', 'born')]\n",
      "!!! target [('He', 'Paris', 'born in')]\n",
      "[('He', 'Lancashire', 'born'), ('He', 'Wigan', 'born')]\n",
      "!!! target [('He', 'Wigan', 'born in')]\n",
      "[('He', 'West Dunbartonshire', 'born'), ('He', 'Renton', 'born')]\n",
      "!!! target [('He', 'Renton, West Dunbartonshire', 'born in')]\n",
      "[('Hiroshi Kikuchi', 'a Japanese author', 'was'), ('Kan Kikuchi', 'kanji', 'uses'), ('Hiroshi Kikuchi', 'born', 'was'), ('his pen name', 'kanji', 'uses')]\n",
      "!!! target [('Kan Kikuchi', 'Takamatsu, Kagawa', 'born in')]\n",
      "[]\n",
      "!!! target [('Jean Ousset', 'Porto', 'born in')]\n",
      "[('He', 'Philadelphia', 'born'), ('James Wright', 'Iowa', 'the University'), ('Philadelphia', 'Iowa', 'the University'), ('Kenyon College', 'Iowa', 'the University'), ('students', 'Iowa', 'the University'), ('He', 'Iowa', 'the University'), ('an accusation', 'Iowa', 'the University'), ('he', 'Iowa', 'the University'), ('Franklin', 'Iowa', 'the University'), ('year', 'Iowa', 'the University'), ('burn', 'Iowa', 'the University'), ('an undergraduate degree', 'Iowa', 'the University'), ('E. L. Doctorow', 'Iowa', 'the University'), ('the army', 'Iowa', 'the University'), ('draft cards', 'Iowa', 'the University'), ('Marshall College', 'Iowa', 'the University'), ('a time', 'Iowa', 'the University'), ('Stanford University', 'Iowa', 'the University'), ('born', 'Iowa', 'the University'), ('a graduate student', 'Iowa', 'the University'), ('Case Western Reserve University', 'Iowa', 'the University'), ('a while', 'Iowa', 'the University'), ('he', 'Philadelphia', 'born'), ('Franklin', 'Philadelphia', 'born'), ('an accusation', 'Philadelphia', 'born'), ('burn', 'Philadelphia', 'born'), ('Marshall College', 'Philadelphia', 'born'), ('students', 'draft cards', 'burn'), ('an undergraduate degree', 'Philadelphia', 'born'), ('E. L. Doctorow', 'Philadelphia', 'born'), ('Iowa', 'Philadelphia', 'born'), ('draft cards', 'Philadelphia', 'born'), ('James Wright', 'Philadelphia', 'born'), ('Case Western Reserve University', 'Philadelphia', 'born'), ('Stanford University', 'Philadelphia', 'born'), ('the army', 'Philadelphia', 'born'), ('a graduate student', 'Philadelphia', 'born'), ('the University', 'Philadelphia', 'born'), ('Kenyon College', 'Philadelphia', 'born'), ('a while', 'Philadelphia', 'born'), ('year', 'Philadelphia', 'born'), ('students', 'Philadelphia', 'born'), ('a time', 'Philadelphia', 'born'), ('He', 'Kenyon College', 'attended'), ('Franklin', 'draft cards', 'burn'), ('Marshall College', 'draft cards', 'burn'), ('Philadelphia', 'draft cards', 'burn'), ('E. L. Doctorow', 'draft cards', 'burn'), ('Case Western Reserve University', 'draft cards', 'burn'), ('Kenyon College', 'draft cards', 'burn'), ('the University', 'draft cards', 'burn'), ('Stanford University', 'draft cards', 'burn'), ('James Wright', 'draft cards', 'burn'), ('the army', 'draft cards', 'burn'), ('he', 'draft cards', 'burn'), ('a time', 'draft cards', 'burn'), ('a graduate student', 'draft cards', 'burn'), ('Iowa', 'draft cards', 'burn'), ('a while', 'draft cards', 'burn'), ('an undergraduate degree', 'draft cards', 'burn'), ('born', 'draft cards', 'burn'), ('year', 'draft cards', 'burn'), ('a time', 'the army', 'serving'), ('He', 'draft cards', 'burn'), ('born', 'Kenyon College', 'attended'), ('an accusation', 'draft cards', 'burn'), ('burn', 'students', 'inciting'), ('an accusation', 'students', 'inciting'), ('E. L. Doctorow', 'the army', 'serving'), ('draft cards', 'the army', 'serving'), ('students', 'the army', 'serving'), ('Kenyon College', 'the army', 'serving'), ('Franklin', 'the army', 'serving'), ('James Wright', 'the army', 'serving'), ('an accusation', 'the army', 'serving'), ('Philadelphia', 'the army', 'serving'), ('year', 'the army', 'serving'), ('Marshall College', 'the army', 'serving'), ('Iowa', 'the army', 'serving'), ('He', 'the army', 'serving'), ('Case Western Reserve University', 'the army', 'serving'), ('burn', 'the army', 'serving'), ('Stanford University', 'the army', 'serving'), ('a graduate student', 'the army', 'serving'), ('Franklin', 'Kenyon College', 'attended'), ('the University', 'the army', 'serving'), ('born', 'the army', 'serving'), ('Philadelphia', 'Kenyon College', 'attended'), ('a while', 'the army', 'serving'), ('burn', 'Kenyon College', 'attended'), ('Marshall College', 'students', 'inciting'), ('Franklin', 'students', 'inciting'), ('Case Western Reserve University', 'Kenyon College', 'attended'), ('Philadelphia', 'students', 'inciting'), ('E. L. Doctorow', 'students', 'inciting'), ('an undergraduate degree', 'the army', 'serving'), ('an accusation', 'Kenyon College', 'attended'), ('Kenyon College', 'students', 'inciting'), ('Case Western Reserve University', 'students', 'inciting'), ('the army', 'students', 'inciting'), ('born', 'students', 'inciting'), ('James Wright', 'students', 'inciting'), ('draft cards', 'Kenyon College', 'attended'), ('Marshall College', 'Kenyon College', 'attended'), ('a time', 'students', 'inciting'), ('a while', 'students', 'inciting'), ('Stanford University', 'students', 'inciting'), ('he', 'students', 'inciting'), ('Stanford University', 'Kenyon College', 'attended'), ('the University', 'students', 'inciting'), ('an undergraduate degree', 'students', 'inciting'), ('Iowa', 'students', 'inciting'), ('a graduate student', 'students', 'inciting'), ('He', 'students', 'inciting'), ('Iowa', 'Kenyon College', 'attended'), ('year', 'students', 'inciting'), ('a graduate student', 'Kenyon College', 'attended'), ('an undergraduate degree', 'Kenyon College', 'attended'), ('E. L. Doctorow', 'Kenyon College', 'attended'), ('a while', 'Kenyon College', 'attended'), ('Franklin', 'Marshall College', 'year'), ('students', 'Kenyon College', 'attended'), ('the army', 'Kenyon College', 'attended'), ('draft cards', 'students', 'inciting'), ('James Wright', 'Kenyon College', 'attended'), ('the University', 'Kenyon College', 'attended'), ('he', 'the army', 'serving'), ('year', 'Kenyon College', 'attended'), ('He', 'James Wright', 'born'), ('He', 'E. L. Doctorow', 'born'), ('he', 'Kenyon College', 'attended'), ('Stanford University', 'Marshall College', 'year'), ('He', 'an accusation', 'born'), ('Iowa', 'Marshall College', 'year'), ('Case Western Reserve University', 'Marshall College', 'year'), ('He', 'burn', 'born'), ('Kenyon College', 'Marshall College', 'year'), ('the University', 'Marshall College', 'year'), ('He', 'Franklin', 'born'), ('a graduate student', 'Marshall College', 'year'), ('Philadelphia', 'Marshall College', 'year'), ('E. L. Doctorow', 'Marshall College', 'year'), ('he', 'Marshall College', 'year'), ('He', 'Marshall College', 'year'), ('the army', 'Marshall College', 'year'), ('James Wright', 'Marshall College', 'year'), ('He', 'a time', 'born'), ('He', 'a while', 'born'), ('He', 'Case Western Reserve University', 'born'), ('He', 'Stanford University', 'born'), ('an undergraduate degree', 'Marshall College', 'year'), ('students', 'Marshall College', 'year'), ('born', 'Marshall College', 'year'), ('He', 'the University', 'born'), ('a time', 'Kenyon College', 'attended'), ('He', 'a graduate student', 'born'), ('He', 'year', 'born'), ('a while', 'Marshall College', 'year'), ('He', 'an undergraduate degree', 'born'), ('He', 'he', 'born'), ('a time', 'Marshall College', 'year'), ('burn', 'Marshall College', 'year'), ('draft cards', 'Marshall College', 'year'), ('a while', 'a graduate student', 'became'), ('he', 'a while', 'Having'), ('he', 'a graduate student', 'became'), ('the army', 'a graduate student', 'became'), ('Franklin', 'a graduate student', 'became'), ('James Wright', 'a graduate student', 'became'), ('Marshall College', 'a graduate student', 'became'), ('draft cards', 'a graduate student', 'became'), ('the University', 'a graduate student', 'became'), ('Kenyon College', 'a graduate student', 'became'), ('students', 'a graduate student', 'became'), ('E. L. Doctorow', 'a graduate student', 'became'), ('Philadelphia', 'a graduate student', 'became'), ('burn', 'a graduate student', 'became'), ('Case Western Reserve University', 'a graduate student', 'became'), ('a time', 'a graduate student', 'became'), ('Iowa', 'a graduate student', 'became'), ('year', 'a graduate student', 'became'), ('an accusation', 'a graduate student', 'became'), ('an undergraduate degree', 'a graduate student', 'became'), ('an accusation', 'Marshall College', 'year'), ('he', 'Stanford University', 'a graduate student'), ('Stanford University', 'a graduate student', 'became'), ('born', 'a graduate student', 'became'), ('Iowa', 'Stanford University', 'a graduate student'), ('James Wright', 'Stanford University', 'a graduate student'), ('an undergraduate degree', 'a while', 'Having'), ('James Wright', 'a while', 'Having'), ('E. L. Doctorow', 'Stanford University', 'a graduate student'), ('the University', 'Stanford University', 'a graduate student'), ('a while', 'Stanford University', 'a graduate student'), ('Philadelphia', 'Stanford University', 'a graduate student'), ('Kenyon College', 'Stanford University', 'a graduate student'), ('E. L. Doctorow', 'a while', 'Having'), ('Kenyon College', 'a while', 'Having'), ('an accusation', 'Stanford University', 'a graduate student'), ('the army', 'Stanford University', 'a graduate student'), ('a time', 'Stanford University', 'a graduate student')]\n",
      "!!! target [('He', 'Philadelphia', 'born in')]\n",
      "[('He', 'Scotland', 'born'), ('He', 'Janet Douglas', 'born'), ('He', 'Forfar', 'born'), ('He', 'David Balfour', 'born'), ('He', 'St.', 'born'), ('He', 'the son', 'born')]\n",
      "!!! target [('He', 'Forfar', 'born in')]\n",
      "[('He', 'West Dunbartonshire', 'born'), ('He', 'Milton', 'born')]\n",
      "!!! target [('He', 'Milton, West Dunbartonshire', 'born in')]\n",
      "[('Kim Westwood', 'an Australian author', 'is'), ('Kim Westwood', 'Sydney', 'born'), ('an Australian author', 'Sydney', 'born')]\n",
      "!!! target [('Kim Westwood', 'Sydney', 'born in')]\n",
      "[]\n",
      "!!! target [('Antoine Faivre', 'Reims', 'born in')]\n",
      "[('She', 'Philadelphia', 'born'), ('She', 'Pennsylvania', 'born')]\n",
      "!!! target [('She', 'Philadelphia, Pennsylvania', 'born in')]\n",
      "[('He', 'Salt Lake City', 'born'), ('He', 'a teacher', 'born'), ('He', 'the street', 'born'), ('He', 'the age', 'born')]\n",
      "!!! target [('He', 'Salt Lake City', 'born in')]\n",
      "[('He', 'Massachusetts', 'born'), ('He', 'Concord', 'born'), ('He', 'Doctor Joseph Lee', 'born')]\n",
      "!!! target [('He', 'Concord, Massachusetts', 'born in')]\n",
      "[('glassed objects', 'glass art', 'see'), ('United States', 'glass art', 'see'), ('glass', 'glass art', 'see'), ('sculptor', 'glass art', 'see'), ('blowing', 'glass art', 'see'), ('Josiah McElheny', 'glass art', 'see'), ('assemblages', 'glass art', 'see'), ('an artist', 'glass art', 'see'), ('born', 'glass art', 'see'), ('his work', 'glass art', 'see'), ('Josiah McElheny', 'an artist', 'is'), ('Josiah McElheny', 'his work', 'is'), ('Josiah McElheny', 'sculptor', 'is')]\n",
      "!!! target [('Josiah McElheny', 'United States', 'born in')]\n",
      "[('Mogliano', 'Macerata', 'the province'), ('Massimo Girotti', 'Macerata', 'the province')]\n",
      "!!! target [('Massimo Girotti', 'Mogliano', 'born in')]\n",
      "[('She', 'Tehran', 'born'), ('She', 'cinema', 'studied'), ('born', 'cinema', 'studied'), ('Tehran', 'cinema', 'studied'), ('she', 'cinema', 'studied'), ('She', 'the Art university', 'born'), ('She', 'she', 'born'), ('She', 'film direction', 'born'), ('the Art university', 'Tehran', 'born')]\n",
      "!!! target [('She', 'Tehran', 'born in')]\n",
      "[('He', 'Surrey', 'born')]\n",
      "!!! target [('He', 'Surrey', 'born in')]\n",
      "[('He', 'London', 'born'), ('He', 'Camberwell', 'born'), ('Camberwell', 'London', 'was')]\n",
      "!!! target [('He', 'Camberwell', 'born in')]\n",
      "[('He', 'Grantham', 'born')]\n",
      "!!! target [('He', 'Grantham', 'born in')]\n",
      "[('He', 'County Limerick', 'born'), ('He', 'Ireland', 'born')]\n",
      "!!! target [('He', 'County Limerick', 'born in')]\n",
      "[('He', 'Prague', 'born'), ('He', 'Vienna', 'studied'), ('born', 'Vienna', 'studied'), ('František Ondříček', 'Vienna', 'studied'), ('Guido Adler', 'Vienna', 'studied'), ('Prague', 'Vienna', 'studied'), ('a composer and music teacher', 'Vienna', 'studied'), ('He', 'Guido Adler', 'born'), ('Guido Adler', 'Prague', 'born'), ('František Ondříček', 'Prague', 'born'), ('He', 'František Ondříček', 'born'), ('a composer and music teacher', 'Prague', 'born'), ('He', 'a composer and music teacher', 'born'), ('Vienna', 'Prague', 'born')]\n",
      "!!! target [('He', 'Prague', 'born in')]\n"
     ]
    }
   ],
   "source": [
    "mono_tr_subset = dataset[dataset.rel == 'P19']\n",
    "precision, recall, f1, pred_dict = compute_logreg_nm(mono_tr_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12195121951219512, 0.8333333333333334, 0.21276595744680848)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Braúnas', 'Brazil', 'a municipality'), ('Minas Gerais', 'Brazil', 'is'), ('the Southeast region', 'Brazil', 'is')]\n",
      "!!! target [('municipality', 'Brazil', 'state'), ('Minas Gerais', 'Brazil', 'state')]\n",
      "[]\n",
      "!!! target [('Mariehamn', 'Finland', 'land')]\n",
      "[('India', 'Karnataka', 'a village'), ('the southern state', 'Karnataka', 'a village')]\n",
      "!!! target [('Karnataka', 'India', 'state')]\n",
      "[('Nebriinae', 'California', 'the US state'), ('Nebria darlingtoni', 'California', 'the US state'), ('ground beetle', 'California', 'the US state'), ('a species', 'California', 'the US state'), ('Nebria darlingtoni', 'ground beetle', 'a species')]\n",
      "!!! target [('California', 'US', 'state')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/61 [00:02<02:58,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel P17 skipped, check soon\n",
      "rel P19 skipped, check soon\n",
      "[]\n",
      "!!! target [('He', 'Erlangen', 'died in')]\n",
      "[('He', 'Michigan', 'died')]\n",
      "!!! target [('He', 'Marshall, Michigan', 'died in')]\n",
      "[('he', 'master', 'an appointment'), ('master', 'preacher', 'became'), ('Kitzingen', 'preacher', 'became'), ('Nuremberg', 'preacher', 'became'), ('the Sebaldus school', 'preacher', 'became'), ('he', 'preacher', 'became'), ('Nuremberg', 'master', 'an appointment'), ('preacher', 'master', 'an appointment'), ('Kitzingen', 'master', 'an appointment'), ('an appointment', 'preacher', 'became'), ('the Sebaldus school', 'master', 'an appointment')]\n",
      "!!! target [('Sebaldus', 'Nuremberg', 'died in')]\n",
      "[]\n",
      "!!! target [('He', 'Graz', 'died in')]\n",
      "[]\n",
      "!!! target [('He', 'Tehran', 'died in')]\n",
      "[]\n",
      "!!! target [('He', 'Saint-Anselme', 'died in')]\n",
      "[]\n",
      "!!! target [('He', 'Rotherham', 'died in')]\n",
      "[('He', 'Tarn', 'born'), ('He', 'Labarthe', 'born'), ('He', 'et', 'born'), ('He', 'Garonne', 'born'), ('He', 'Montauban', 'born')]\n",
      "!!! target [('He', 'Montauban', 'died in')]\n",
      "[('He', 'Lausanne', 'died')]\n",
      "!!! target [('He', 'Pully', 'died in')]\n",
      "[]\n",
      "!!! target [('She', 'London', 'died in')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 3/61 [00:04<01:09,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "!!! target [('He', 'Caserta', 'died in')]\n",
      "rel P20 skipped, check soon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/61 [00:04<00:34,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel P22 skipped, check soon\n",
      "rel P25 skipped, check soon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 7/61 [00:04<00:19,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel P26 skipped, check soon\n",
      "[('Sogn', 'og', 'the county'), ('Fjordane', 'og', 'the county')]\n",
      "!!! target [('Sogn og Fjordane', 'county', 'is a')]\n",
      "rel P31 skipped, check soon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 9/61 [00:05<00:28,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel P36 skipped, check soon\n",
      "[('Laodamas', 'the son', 'kills'), ('Laodamas', 'Eteocles', 'the son')]\n",
      "!!! target [('Eteocles', 'Laodamas', 'son')]\n",
      "rel P40 skipped, check soon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-938849432acd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_csv_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-106331c576a5>\u001b[0m in \u001b[0;36mcompute_csv_default\u001b[0;34m(dataset, sample_size, filename)\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mmono_tr_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmono_tr_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmono_tr_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-106331c576a5>\u001b[0m in \u001b[0;36mget_title\u001b[0;34m(relation_id)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelation_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mRELATIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRELATIONS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelation\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mrelation_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "compute_csv_default(dataset, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
