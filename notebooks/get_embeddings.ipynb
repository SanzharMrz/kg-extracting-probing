{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9ggK4kSb5eU",
    "outputId": "277ec815-a768-4739-feb8-b5e79e866295"
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download en\n",
    "#!pip install --quiet transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "45XGTF9sxoi_"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import product\n",
    "from IPython.display import display\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "# import fasttext as ft\n",
    "import en_core_web_sm\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q7nlxSDhiuY7",
    "outputId": "2da48fcb-dbcc-4a88-e73f-7b291bb99e67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Анастасия\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Анастасия\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation += '’'\n",
    "string.punctuation += '–'\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_model = ft.load_model('../language-models-are-knowledge-graphs-pytorch/similarity/LMMS/cc.en.300.bin')\n",
    "#ft_model = ft.load_model('path/to/model/cc.en.300.bin')\n",
    "nlp = en_core_web_sm.load()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ky1gZnF0zTyN"
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = bert_model.eval()\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "encoder = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "encoder = encoder.eval()\n",
    "encoder = encoder.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_vector(text, rel=None):\n",
    "    encoded_input = bert_tokenizer(text, return_tensors='pt').to(device)  \n",
    "    output = bert_model(**encoded_input)\n",
    "    logits = output[0].squeeze()[1:-1]\n",
    "    \n",
    "    default_return = torch.mean(logits, axis=0).detach().cpu().numpy()\n",
    "    if not rel:\n",
    "        return default_return\n",
    "    \n",
    "    encoded_rel = bert_tokenizer(rel, return_tensors='pt')\n",
    "\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(encoded_input['input_ids'].squeeze())[1:-1]\n",
    "    \n",
    "    rel_tokens = bert_tokenizer.convert_ids_to_tokens(encoded_rel['input_ids'].squeeze()[1:-1])\n",
    "    indices = []\n",
    "    for rel_tok in rel_tokens:\n",
    "        try:\n",
    "            r_t = tokens.index(rel_tok)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                r_t = tokens.index('##' + rel_tok)\n",
    "            except:\n",
    "                continue \n",
    "        indices.append(r_t)\n",
    "    if len(indices):\n",
    "        return torch.mean(logits[indices,:], axis=0).detach().cpu().numpy()\n",
    "\n",
    "    return default_return\n",
    "\n",
    "\n",
    "def vectorize_pred_rel(text, rel_pred):\n",
    "    pred_vector = np.concatenate((ft_model.get_sentence_vector(rel_pred), get_bert_vector(text, rel_pred), get_bert_vector(text, rel_pred)))\n",
    "    return pred_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "ErL0Tce6B3CG"
   },
   "outputs": [],
   "source": [
    "def get_embs_for_triplets(triplets, sentence_mapping, attention, attentions_types, with_label=False, use_bert=False, use_lmms=False):\n",
    "    sent_embeddings = []\n",
    "      \n",
    "    for triplet in triplets:\n",
    "        #  tokenize target the same way as sentence to avoid index error\n",
    "        if ',' in triplet[0] or \"'\" in triplet[0]:\n",
    "            head = ' '.join(word_tokenize(triplet[0]))\n",
    "        else:\n",
    "            head = triplet[0]\n",
    "      \n",
    "        if ',' in triplet[1] or \"'\" in triplet[1]:\n",
    "            tail = ' '.join(word_tokenize(triplet[1]))\n",
    "        else:\n",
    "            tail = triplet[1]\n",
    "      \n",
    "        if ',' in triplet[2] or \"'\" in triplet[2]:\n",
    "            rel = ' '.join(word_tokenize(triplet[2]))\n",
    "        else:\n",
    "            rel = triplet[2]\n",
    "        \n",
    "        try:\n",
    "            if head in sentence_mapping and tail in sentence_mapping and rel in sentence_mapping:\n",
    "                #  get head, tail, rel indices in the matrix (len(sentence_mapping) == len(att_matrix)) \n",
    "                head_ind = sentence_mapping.index(head)\n",
    "                tail_ind = sentence_mapping.index(tail)\n",
    "                rel_ind = sentence_mapping.index(rel)   \n",
    "                \n",
    "                #  get vector of attention from every head\n",
    "                head_rel_emb = attention[:, head_ind, rel_ind]\n",
    "                rel_tail_emb = attention[:, rel_ind, tail_ind]\n",
    "                head_tail_emb = attention[:, head_ind, tail_ind]\n",
    "                rel_head_emb = attention[:, rel_ind, head_ind]\n",
    "                tail_rel_emb = attention[:, tail_ind, rel_ind]\n",
    "                tail_head_emb = attention[:, tail_ind, head_ind]\n",
    "                #  LMMS vectorization here\n",
    "                \n",
    "                #  choose only neede vectors\n",
    "                attentions_to_be_used = [head_rel_emb, rel_tail_emb, head_tail_emb, rel_head_emb, tail_rel_emb, tail_head_emb] \n",
    "                attentions_to_use = tuple([att for i, att in enumerate(attentions_to_be_used) if attentions_types[i] == 1])\n",
    "\n",
    "                #  concat chosen vectors into one\n",
    "                triplet_emb = np.concatenate(attentions_to_use, axis=0).squeeze()\n",
    "                sentence = ' '.join(sentence_mapping)\n",
    "                if use_bert:\n",
    "                    extra_embedding = get_bert_vector(sentence, rel)\n",
    "                    triplet_emb = np.concatenate((triplet_emb, extra_embedding))\n",
    "                    \n",
    "                if use_lmms:\n",
    "                    extra_embedding = vectorize_pred_rel(sentence, rel)\n",
    "                    triplet_emb = np.concatenate((triplet_emb, extra_embedding))\n",
    "                    \n",
    "                #  add label if 'train' \n",
    "                if with_label:\n",
    "                    rel_label = triplet[3]\n",
    "                    sent_embeddings.append((triplet_emb, triplet, rel_label))\n",
    "                else:\n",
    "                    sent_embeddings.append((triplet_emb, triplet))\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return sent_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "Ga3-5NbF_oy6"
   },
   "outputs": [],
   "source": [
    "def return_embeddings(sentence, attentions_types, tokenizer, encoder, nlp, use_cuda, use_bert, use_lmms, target=None, mode='train'):\n",
    "   \n",
    "    tokenizer_name = str(tokenizer.__str__)\n",
    "    rel_pos = ['NN', 'NNP', 'NNS', 'MD', 'POS', 'VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ']\n",
    "    head_tail_pos = ['NN', 'NNP', 'NNS', 'PRP']\n",
    "\n",
    "    if mode == 'train':\n",
    "        #  to process data with rel labels from dataset (pass target)\n",
    "        inputs, tokenid2word_mapping, token2id, sentence_mapping = create_mapping_target(sentence, \n",
    "                                                                                         target, \n",
    "                                                                                         return_pt=True, \n",
    "                                                                                         tokenizer=tokenizer)\n",
    "    \n",
    "    else:\n",
    "        #  to process data to predict\n",
    "        inputs, tokenid2word_mapping, token2id, sentence_mapping, noun_chunks = create_mapping(sentence, \n",
    "                                                                                               return_pt=True, \n",
    "                                                                                               nlp=nlp,\n",
    "                                                                                               tokenizer=tokenizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if use_cuda:\n",
    "            for key in inputs.keys():\n",
    "                inputs[key] = inputs[key].cuda()\n",
    "        try:\n",
    "            outputs = encoder(**inputs, output_attentions=True)\n",
    "        except RuntimeError:\n",
    "            print(sentence_mapping)\n",
    "            return []\n",
    "\n",
    "    attn = outputs[2]   \n",
    "\n",
    "    new_matr = []\n",
    "    \n",
    "    for layer in attn:\n",
    "        for head in layer.squeeze():\n",
    "            if use_cuda:\n",
    "                attn = head.cpu()\n",
    "            else:\n",
    "                attn = head\n",
    "            attention_matrix = attn.detach().numpy()\n",
    "            attention_matrix = attention_matrix[1:-1, 1:-1]\n",
    "            merged_attention = compress_attention(attention_matrix, tokenid2word_mapping)\n",
    "            new_matr.append(merged_attention)\n",
    "\n",
    "    new_matr = np.array(new_matr)\n",
    "    \n",
    "    #  get candidates for head, tail and rel\n",
    "    words = [token for token in sentence_mapping if token not in string.punctuation]\n",
    "    nn_words = [word for word in words if nltk.pos_tag([word])[0][1] in head_tail_pos]\n",
    "    other_words = [word for word in words if nltk.pos_tag([word])[0][1] in rel_pos]\n",
    "    \n",
    "    sent_embeddings = []\n",
    "\n",
    "    if mode == 'train':\n",
    "        #  get candidate triplets (in this case - for 'garbage class')\n",
    "        triplets = [triplet for triplet in list(product(nn_words, nn_words, other_words)) \n",
    "                        if triplet[0] != triplet[1] and triplet[0] != triplet[2] and triplet[1] != triplet[2] and triplet not in target]\n",
    "        other_triplets = [(t[0], t[1], t[2], '0') for t in triplets]\n",
    "        \n",
    "        #  get embeddings for 'garbage' class\n",
    "        try:\n",
    "            sent_embeddings.extend(get_embs_for_triplets(random.choices(other_triplets, k=len(target)), sentence_mapping, new_matr, attentions_types, \n",
    "                                                        with_label=True, \n",
    "                                                        use_bert=use_bert,\n",
    "                                                        use_lmms=use_lmms))\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        #  get embeddings for target class\n",
    "        sent_embeddings.extend(get_embs_for_triplets(target, sentence_mapping, new_matr, attentions_types, \n",
    "                                                     with_label=True, \n",
    "                                                     use_bert=use_bert, \n",
    "                                                     use_lmms=use_lmms))\n",
    "\n",
    "        \n",
    "    else:\n",
    "        #  get candidate triplets from the sentence\n",
    "        triplets = [triplet for triplet in list(product(nn_words, nn_words, other_words)) \n",
    "                      if triplet[0] != triplet[1] and triplet[0] != triplet[2] and triplet[1] != triplet[2]]\n",
    "        \n",
    "        #  get embeddings for candidate triplets (to be classified further)\n",
    "        sent_embeddings.extend(get_embs_for_triplets(triplets, sentence_mapping, new_matr, attentions_types, with_label=False))\n",
    "    \n",
    "    return sent_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(data_type, attentions_types, use_bert, use_lmms):\n",
    "    attentions_to_be_used = ['h-r', 'r-t', 'h-t', 'r-h', 't-r', 't-h'] \n",
    "    attentions_to_use = tuple([att for i, att in enumerate(attentions_to_be_used) if attentions_types[i] == 1])\n",
    "    att_names = '_'.join(attentions_to_use)\n",
    "    name = f'{data_type}_{att_names}'\n",
    "    \n",
    "    if use_bert:\n",
    "        name += '_bert'\n",
    "    \n",
    "    if use_lmms:\n",
    "        name += '_lmms'\n",
    "        \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "LlCP2XCOeX9J"
   },
   "outputs": [],
   "source": [
    "def get_embeddings_corpus(data_type, attentions_types, use_bert, use_lmms):  \n",
    "    use_cuda = True\n",
    "    data = pd.read_csv(f'../data/train-val-test/{data_type}.csv', header=0)\n",
    "    os.makedirs('./vectors/', exist_ok=True)\n",
    "\n",
    "    all_embeddings = []\n",
    "    \n",
    "    if data_type == 'train' or data_type == 'test':\n",
    "        mode = 'train'\n",
    "    else:\n",
    "        mode = 'valid'\n",
    "        \n",
    "    for ind, row in tqdm(data.iterrows(), total=data.shape[0], desc=f'Getting embeddings for {data_type}'):\n",
    "        text = row['text']\n",
    "        target = eval(row['target'])\n",
    "        embeddings_text = return_embeddings(text, \n",
    "                                       attentions_types, \n",
    "                                       tokenizer, \n",
    "                                       encoder, \n",
    "                                       nlp, \n",
    "                                       use_cuda, \n",
    "                                       use_bert=use_bert, \n",
    "                                       use_lmms=use_lmms, \n",
    "                                       target=target, \n",
    "                                       mode=mode)\n",
    "        \n",
    "        all_embeddings.extend(embeddings_text)\n",
    "        \n",
    "        file_name = get_filename(data_type, attentions_types, use_bert, use_lmms)\n",
    "        \n",
    "#         df = pd.DataFrame(embeddings_text)\n",
    "#         df.to_csv(f'./vectors/{csv_name}.csv', mode='a', header=False, index=False)\n",
    "    \n",
    "    with open(f'./vectors/{file_name}.pkl', 'wb') as file:\n",
    "        pickle.dump(all_embeddings, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attentions_to_be_used = [head_rel, rel_tail, head_tail, rel_head, tail_rel, tail_head]\n",
    "\n",
    "attentions_types_example = [1, 1, 0, 0, 0, 0] + use_bert + use_lmms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(attentions_types, use_bert, use_lmms):\n",
    "    get_embeddings_corpus('train', attentions_types, use_bert, use_lmms)\n",
    "    get_embeddings_corpus('test', attentions_types, use_bert, use_lmms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0d1e91db53496ea0d2456f0bab4f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting embeddings for train'), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767a9b1615754debbc5e34e2eb4ab2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting embeddings for test'), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288805de7a4749d2a24a2b6d2674c571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting embeddings for train'), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e493734f6c4b87a03b5e86093b9c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Getting embeddings for test'), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#  не все варианты, а которые, кажется, нужно попробовать в первую очередь\n",
    "combinations = [([1, 1, 0, 0, 0, 0], False, False),\n",
    "                ([1, 1, 0, 0, 0, 0], True, False),]\n",
    "#                 ([1, 1, 0, 0, 0, 0], False, True),\n",
    "#                 ([1, 1, 0, 0, 0, 0], True, True),\n",
    "#                 ([1, 1, 1, 0, 0, 0], False, False),\n",
    "#                 ([1, 1, 1, 0, 0, 0], True, False),\n",
    "#                 ([1, 1, 1, 0, 0, 0], False, True),\n",
    "#                 ([1, 1, 1, 0, 0, 0], True, True)]\n",
    "\n",
    "for comb in combinations:\n",
    "    attentions_types, use_bert, use_lmms = comb\n",
    "    get_train_test_data(attentions_types, use_bert, use_lmms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "get_attention_embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "294c464a9e0248e39eb66505e5ba40ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75ae982e567e41c293522f5e1bcb9d68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd3a2f593cff44a19881cb7f92cdebd4",
      "placeholder": "​",
      "style": "IPY_MODEL_fb38bd02eab148d7ab5647ca3f9eeea7",
      "value": " 10/10 [00:01&lt;00:00,  6.34it/s]"
     }
    },
    "a42fedea067d4fcf9e1a5adb2d6b67e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_294c464a9e0248e39eb66505e5ba40ef",
      "placeholder": "​",
      "style": "IPY_MODEL_e30d4b963d9340a49af3765ae2e1cff0",
      "value": "100%"
     }
    },
    "a8d3da9bbcc84ed99b29734efb920e46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b027375fd8444100acd07b5b0941ca80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a42fedea067d4fcf9e1a5adb2d6b67e0",
       "IPY_MODEL_e9066b4128cc42cab97bb966dd5d06fc",
       "IPY_MODEL_75ae982e567e41c293522f5e1bcb9d68"
      ],
      "layout": "IPY_MODEL_a8d3da9bbcc84ed99b29734efb920e46"
     }
    },
    "dd3a2f593cff44a19881cb7f92cdebd4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e30d4b963d9340a49af3765ae2e1cff0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9066b4128cc42cab97bb966dd5d06fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2945699b5754faeb743f823ea220006",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ed66e941257a4664b785a891d3e86097",
      "value": 10
     }
    },
    "ed66e941257a4664b785a891d3e86097": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2945699b5754faeb743f823ea220006": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb38bd02eab148d7ab5647ca3f9eeea7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
